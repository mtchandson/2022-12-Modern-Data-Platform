## JDBC driver

This section presents the steps to configure your JDBC driver to connect to Azure Databricks.

### Install the Databricks JDBC driver in a Java project

The Databricks JDBC driver is available in the [Maven Central repository](https://search.maven.org/artifact/com.databricks/databricks-jdbc). To include the Databricks JDBC driver in your Java project, add the following entry to your application’s `pom.xml` file, as follows.

 Note

The following `version` value is subject to change. For available versions to choose from, see the [Maven Central repository](https://search.maven.org/artifact/com.databricks/databricks-jdbc).

XMLCopy

```
<dependency>
  <groupId>com.databricks<ʇgroupId>
  <artifactId>databricks-jdbc</artifactId>
  <version>2.6.25-1</version>
</dependency>

```

### Download the Databricks JDBC driver

1. Go to the [Databricks JDBC driver](https://databricks.com/spark/jdbc-drivers-download) download page to download the driver.
2. The driver is packaged as a JAR, which does not require installation and can be added to the Java classpath.

Since **JDBC 2.6.25** the driver name is `DatabricksJDBC42.jar`, whereas the legacy driver’s name is `SparkJDBC42.jar`. After you download the driver, use the following instructions to configure the driver:

- [Building the connection URL for the Databricks JDBC driver](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#databricks-jdbc)
- [Building the connection URL for the legacy Spark driver](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#spark-jdbc)

### Building the connection URL for the Databricks driver

To connect using a personal access token, first get the **Server Hostname** and **Http Path** from [Retrieve the connection details](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#retrieve-the-connection-details).

The JDBC connection URL has the following general form:

Copy

```
jdbc:databricks://<Server Hostname>:443;HttpPath=<Http Path>[;property=value[;property=value]]

```

where:

- **jdbc:databricks://** (Required) is known as the subprotocol and is constant.
- **Server Hostname** (Required) is the address of the server to connect to.
- **Http Path** (Required) is the Azure Databricks compute resources URL.
- **property** (Optional) is one or more connection properties. See [JDBC driver capabilities](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#jdbc-driver-capabilities) for more details.

You should avoid setting credentials via the JDBC URL. Instead, the recommended way of setting credentials is to pass them through the properties parameter to the `DriverManager`:

To authenticate using a personal access token, set the following properties collection:

Copy

```
String url = "jdbc:databricks://<Server Hostname>:443;HttpPath=<Http Path>";
Properties p = new java.util.Properties();
p.put("PWD", "<personal-access-token>");
DriverManager.getConnection(url, p);

```

where:

- **PWD** is the personal access token that you obtained in [Authentication requirements](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#authentication).

To authenticate with an Azure Active Directory token, set the following properties collection:

Copy

```
String url = "jdbc:databricks://<Server Hostname>:443;HttpPath=<Http Path>";
Properties p = new java.util.Properties();
p.put("AuthMech", "11");
p.put("Auth_Flow", "0");
p.put("Auth_AccessToken", "<aad-token>");
p.put("PWD", "<persoanl-access-token>");
DriverManager.getConnection(url, p);

```

where:

- **Auth_AccessToken** is the Azure AD token that you obtained in [Authentication requirements](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#authentication).

### Building the connection URL for the legacy Spark driver

To connect to Azure Databricks using the Spark JDBC driver you need to build a connection URL that has the following general form:

Copy

```
jdbc:spark://<Server Hostname>:443;HttpPath=<Http Path>;TransportMode=http;SSL=1[;property=value[;property=value]]

```

where:

- **jdbc:spark://** (Required) is known as the subprotocol and is constant.
- **Server Hostname** (Required) is the address of the server to connect to.
- **Http Path** (Required) is the Azure Databricks compute resources URL.
- **property** (Optional) is one or more connection properties. See [JDBC driver capabilities](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#jdbc-driver-capabilities) for more details.

The driver also requires setting **TransportMode** and **SSL** properties. The Databricks recommended values of these properties are `http` and 1, respectively.

To authenticate using a personal access token, set the following properties collection:

Copy

```
String url = "jdbc:spark://<Server Hostname>;HttpPath=<Http Path>;TransportMode=http;SSL=1";
Properties p = new java.util.Properties();
p.put("PWD", "<personal-access-token>");
DriverManager.getConnection(url, p);

```

where:

- **PWD** is the personal access token that you obtained in [Authentication requirements](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#authentication).

To authenticate with an Azure Active Directory token, set the following properties collection:

Copy

```
String url = "jdbc:spark://<Server Hostname>;HttpPath=<Http Path>;TransportMode=http;SSL=1";
Properties p = new java.util.Properties();
p.put("AuthMech", "11");
p.put("Auth_Flow", "0");
p.put("Auth_AccessToken", "<aad-token>");
p.put("PWD", "<persoanl-access-token>");
DriverManager.getConnection(url, p);

```

where:

- **Auth_AccessToken** is the Azure AD token that you obtained in [Authentication requirements](https://learn.microsoft.com/en-us/azure/databricks/integrations/jdbc-odbc-bi#authentication).

### JDBC driver capabilities

This section presents optional JDBC driver configurations. The same capabilities apply to both Databricks and legacy Spark drivers.

#### ANSI SQL-92 query support in JDBC

Legacy Spark JDBC drivers accept SQL queries in ANSI SQL-92 dialect and translate the queries to the Databricks SQL dialect before sending them to the server. However, if your application generates Databricks SQL directly or your application uses any non-ANSI SQL-92 standard SQL syntax specific to Azure Databricks, Databricks recommends that you set `UseNativeQuery=1` as a connection configuration. With that setting, the driver passes the SQL queries verbatim to Azure Databricks.

#### Extract large query results in JDBC

To achieve the best performance when you extract large query results, use the latest version of the JDBC driver, which includes the following optimizations.

##### Arrow serialization in JDBC

The JDBC driver version 2.6.16 and above supports an optimized query results serialization format that uses [Apache Arrow](https://arrow.apache.org/docs/index.html).

##### Cloud Fetch in JDBC

The JDBC driver version 2.6.19 and above supports Cloud Fetch, a capability that fetches query results through the cloud storage that is set up in your Azure Databricks deployment. To use Cloud Fetch to extract query results, use Databricks Runtime 8.3 or above.

Query results are uploaded to an internal [DBFS storage location](https://learn.microsoft.com/en-us/azure/databricks/dbfs/) as Arrow-serialized files of up to 20 MB. When the driver sends fetch requests after query completion, Azure Databricks generates and returns [shared access signatures](https://learn.microsoft.com/en-us/https://learn.microsoft.com/storage/common/storage-sas-overview) to the uploaded files. The JDBC driver then uses the URLs to download the results directly from DBFS.

Cloud Fetch is only used for query results larger than 1 MB. Smaller results are retrieved directly from Azure Databricks.

Azure Databricks automatically garbage collects the accumulated files which are marked for deletion after 24 hours. These marked files are completely deleted after an additional 24 hours.

To learn more about the Cloud Fetch architecture, see [How We Achieved High-bandwidth Connectivity With BI Tools](https://databricks.com/blog/2021/08/11/how-we-achieved-high-bandwidth-connectivity-with-bi-tools.html).

#### JDBC driver guide

For more information about the JDBC driver, refer to the installation and configuration guide. Find the Databricks JDBC driver installation and configuration guide in the `docs` directory of the driver package.

## 